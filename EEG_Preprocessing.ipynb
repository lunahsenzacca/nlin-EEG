{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a77f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne  # For EEG/MEG data processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mne.io import read_raw_edf, read_raw_fif, read_raw_bdf\n",
    "from mne.preprocessing import ICA\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "def ion():\n",
    "    mne.viz.set_browser_backend('qt', verbose = True)\n",
    "    return\n",
    "\n",
    "def ioff():\n",
    "    mne.viz.set_browser_backend('matplotlib', verbose = False)\n",
    "    return\n",
    "\n",
    "# proc_info.csv dict keys\n",
    "#'SubID';'Filters';'bad_ICA';'bad_CH';'TRIG_swap';'reject';'flat';'bad_Trials';'Comment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0390f",
   "metadata": {},
   "source": [
    "# Load data and basic informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3895b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main path that will be used for saving\n",
    "main_path = '/home/lunis/Documents/nlin-EEG/data/BMD'\n",
    "\n",
    "# Subject ID\n",
    "subj = '038'\n",
    "\n",
    "# Subject unprocessed raw header location\n",
    "file_path = main_path + 'Unprocessed/subj' + subj + '.vhdr'\n",
    "\n",
    "# Load the EEG data from the .vhdr file\n",
    "raw = mne.io.read_raw_brainvision(file_path, preload=True)\n",
    "\n",
    "# Print general information about the loaded data\n",
    "print(raw.info)\n",
    "\n",
    "# Drop Iz\n",
    "raw.drop_channels(['Iz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d675bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch_name, ch_type in zip(raw.info['ch_names'], raw.get_channel_types()):\n",
    "    print(f\"Channel: {ch_name}, Type: {ch_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb376a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling rate (Hz)\n",
    "print(f'Sampling rate: {raw.info[\"sfreq\"]} Hz')\n",
    "\n",
    "# List of channels (e.g., Fp1, Fp2, Cz, etc.)\n",
    "print(f'Channels: {raw.ch_names}')\n",
    "\n",
    "# Duration of the recording (in seconds)\n",
    "duration = raw.n_times / raw.info['sfreq']\n",
    "print(f'Recording duration: {duration} seconds')\n",
    "\n",
    "# Number of EEG channels\n",
    "n_channels = len(raw.ch_names)\n",
    "print(f'Number of channels: {n_channels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796332bd",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aaab38",
   "metadata": {},
   "source": [
    "##  Detect long breaks in the recordings and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4529ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_annots = mne.preprocessing.annotate_break(\n",
    "    raw = raw,\n",
    "    min_break_duration = 20,  # consider segments of at least 20 s duration\n",
    "    t_start_after_previous = 2,  # start annotation 2 s after end of previous one\n",
    "    t_stop_before_next = 2,  # stop annotation 2 s before beginning of next one\n",
    ");\n",
    "\n",
    "# Create complementary annotation\n",
    "n_break_annots_onsets = [break_annots[i]['onset'] + break_annots[i]['duration'] for i in range(0,len(break_annots))]\n",
    "\n",
    "n_break_annots_durations = [break_annots[i + 1]['onset'] - n_break_annots_onsets[i] for i in range(0,len(break_annots) - 1)] + [0]\n",
    "\n",
    "# Delete segments too short for the notch filter\n",
    "for i, dur in enumerate(n_break_annots_durations):\n",
    "\n",
    "    if dur < 6.6:\n",
    "        n_break_annots_onsets.pop(i)\n",
    "        n_break_annots_durations.pop(i)\n",
    "\n",
    "n_break_annots = mne.Annotations(onset = n_break_annots_onsets,\n",
    "                                 duration = n_break_annots_durations,\n",
    "                                 description = break_annots[0]['description'],\n",
    "                                 orig_time = break_annots[0]['orig_time'],\n",
    "                                 extras = None)\n",
    "\n",
    "if len(n_break_annots) != 0:\n",
    "\n",
    "    raw.set_annotations(raw.annotations + n_break_annots)\n",
    "\n",
    "    raw_segments = raw.crop_by_annotations(n_break_annots)\n",
    "\n",
    "    raw_cropped = mne.concatenate_raws(raw_segments)\n",
    "\n",
    "    del raw_segments\n",
    "\n",
    "else:\n",
    "\n",
    "    raw_cropped = raw.copy()\n",
    "\n",
    "del raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove inherited annotations (There has to be a better way...)\n",
    "delete = []\n",
    "for i in range(0,len(raw_cropped.annotations)):\n",
    "\n",
    "    if raw_cropped.annotations[i]['description'] == 'BAD_break':\n",
    "        delete.append(i)\n",
    "\n",
    "raw_cropped.annotations.delete(delete)\n",
    "#raw_cropped.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7bf26",
   "metadata": {},
   "source": [
    "## Basic filtering and rereferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the EEG data (remove frequencies below 1 Hz and above 40 Hz)\n",
    "bandpass = [2.0,100.0]\n",
    "notches = [50,100]\n",
    "\n",
    "raw_filtered = raw_cropped.copy().filter(l_freq=bandpass[0], h_freq=bandpass[1])\n",
    "\n",
    "# Remove AC interference\n",
    "raw_filtered.notch_filter(freqs = [50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PRE and POST filter raw signals side by side\n",
    "ioff()\n",
    "pre = raw_cropped.plot(n_channels = 61, start = 250, show = False);\n",
    "plt.close()\n",
    "post = raw_filtered.plot(n_channels = 61, start = 250, show = False);\n",
    "plt.close()\n",
    "\n",
    "pre_canvas = FigureCanvas(pre)\n",
    "pre_canvas.draw ()\n",
    "\n",
    "pre_img = np.asarray(pre_canvas.buffer_rgba())\n",
    "\n",
    "post_canvas = FigureCanvas(post)\n",
    "post_canvas.draw ()\n",
    "\n",
    "post_img = np.asarray(post_canvas.buffer_rgba())\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (20,20), dpi = 300)\n",
    "\n",
    "ax[0].imshow(pre_img)\n",
    "ax[1].imshow(post_img)\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "ax[0].set_title('PRE-Filtering')\n",
    "ax[1].set_title('POST-Filtering')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353909ad",
   "metadata": {},
   "source": [
    "## ICA preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = ICA(n_components=30, random_state=42, max_iter='auto')\n",
    "ica.fit(raw_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8426a8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Print Power spectrum of ICA decomposition ###\n",
    "sources = ica.get_sources(raw_filtered)\n",
    "\n",
    "# Plot the power spectrum for each ICA component\n",
    "n_components = ica.n_components_ \n",
    "\n",
    "# Loop through each ICA component\n",
    "for i in range(n_components):\n",
    "    # Extract the signal of component i\n",
    "    component_data = sources.get_data(picks=[i])  # Get data for the i-th ICA component\n",
    "\n",
    "    # Compute the power spectral density (PSD) of the component\n",
    "    psd, freqs = mne.time_frequency.psd_array_welch(\n",
    "        component_data[0],  # Extract the first row (since it's a single component)\n",
    "        sfreq=raw_cropped.info['sfreq'],  # Sampling frequency from the raw data\n",
    "        fmin=bandpass[0], fmax=bandpass[1],  # Focus on the filter range\n",
    "        n_fft=2048,  # Length of FFT (controls frequency resolution)\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    # Plot the power spectrum of the component\n",
    "    fig, ax = plt.subplots(1,2, figsize = (7,3), dpi = 100)\n",
    "    ax[1].plot(freqs, 10 * np.log10(psd), label=f'Component {i}')\n",
    "    ax[1].set_xlabel('Frequency (Hz)')\n",
    "    ax[1].set_ylabel('Power (dB)')\n",
    "    ax[1].set_title(f'Power Spectrum of ICA Component {i}')\n",
    "    ax[1].grid(True, linestyle='--', alpha=0.6)\n",
    "    ax[1].set_ylim(-35,-6)\n",
    "\n",
    "    ica.plot_components(picks = i, show_names = True, axes = ax[0]);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4942bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sources in interactive mode for bad sources selection\n",
    "ion()\n",
    "ica.plot_sources(raw_filtered, start = 250, precompute = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b155c58",
   "metadata": {},
   "source": [
    "### Remove bad components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what components where excluded in interactive mode\n",
    "bad_ICA = sorted(ica.exclude)\n",
    "print(bad_ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687182d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ICA to copy of raw data\n",
    "cleaned_raw = raw_filtered.copy()\n",
    "ica.apply(cleaned_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d01190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PRE and POST ICA raw signals side by side\n",
    "ioff()\n",
    "pre = raw_filtered.plot(n_channels = 61, start = 250, show = False);\n",
    "plt.close()\n",
    "post = cleaned_raw.plot(n_channels = 61, start = 250, show = False);\n",
    "plt.close()\n",
    "\n",
    "pre_canvas = FigureCanvas(pre)\n",
    "pre_canvas.draw ()\n",
    "\n",
    "pre_img = np.asarray(pre_canvas.buffer_rgba())\n",
    "\n",
    "post_canvas = FigureCanvas(post)\n",
    "post_canvas.draw ()\n",
    "\n",
    "post_img = np.asarray(post_canvas.buffer_rgba())\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (20,20), dpi = 300)\n",
    "\n",
    "ax[0].imshow(pre_img)\n",
    "ax[1].imshow(post_img)\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "ax[0].set_title('PRE-ICA Removal')\n",
    "ax[1].set_title('POST-ICA Removal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open POST ICA raw in interactive mode\n",
    "ion()\n",
    "cleaned_raw.plot(n_channels = 61, start = 250, precompute = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be350f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interpolated channels names and apply interpolation\n",
    "bad_CH = cleaned_raw.info['bads']\n",
    "\n",
    "for i, b in enumerate(bad_CH):\n",
    "    bad_CH[i] = str(b)\n",
    "\n",
    "if len(bad_CH) != 0:\n",
    "    cleaned_raw.interpolate_bads()\n",
    "    ioff()\n",
    "    cleaned_raw.plot(n_channels = 61, start = 250);\n",
    "\n",
    "    print(f'Channels {bad_CH} were interpolated')\n",
    "\n",
    "else:\n",
    "    print('No bad channels were selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdb319",
   "metadata": {},
   "source": [
    "# Extract the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract event markers\n",
    "events, event_dict = mne.events_from_annotations(raw_cropped) # events : n_events x ? x event code\n",
    "\n",
    "# Print all the event names and codes\n",
    "print(f'Event dictionary: {event_dict}')\n",
    "\n",
    "# Keep only the ones we are interested in\n",
    "red_event_dict = {'S__1': 1,\n",
    "                  'S__2': 2,\n",
    "                  'S__3': 3,\n",
    "                  'S__4': 4,\n",
    "                  'S_11': 11,\n",
    "                  'S_12': 12,\n",
    "                  'S_13': 13,\n",
    "                  'S_14': 14}\n",
    "\n",
    "# Print reduced event names and codes\n",
    "print(f'Reduced event dictionary: {red_event_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Maksing Triggers\n",
    "\n",
    "# 1 Conscious Left, Self\n",
    "# 2 Conscious Left, Other\n",
    "# 3 Conscious Right, Self\n",
    "# 4 Conscious Right, Other\n",
    "\n",
    "#11 Unconscious Left, Self\n",
    "#12 Unconscious Left, Other\n",
    "#13 Unconscious Right, Self\n",
    "#14 Unconscious Right, Other\n",
    "\n",
    "#20 Correct Localization Answer (not interested)\n",
    "#21 Incorrect Localization Answer (not interested)\n",
    "#30 Correct Discrimination Answer (not interested)\n",
    "#31 Incorrect Discrimination Answer (not interested)\n",
    "\n",
    "#40 Invisible\n",
    "#41 Almost invisible\n",
    "#42 Barely Visible\n",
    "#43 Visible\n",
    "\n",
    "# Make Trial Conscious or Unconscious based on Answer\n",
    "\n",
    "# Create copy of events array\n",
    "m_events = np.copy(events)\n",
    "\n",
    "vis_masked = 0\n",
    "invis_unmasked = 0\n",
    "for i, event in enumerate(events):\n",
    "\n",
    "    # If visible\n",
    "    if event[2] in [43]:\n",
    "        if m_events[i - 3][2] in [11,12,13,14]:\n",
    "            m_events[i - 3][2] = m_events[i - 3][2] - 10\n",
    "            vis_masked += 1\n",
    "\n",
    "    # If not visible\n",
    "    elif event[2] in [40]:\n",
    "        if m_events[i - 3][2] in [1,2,3,4]:\n",
    "            m_events[i - 3][2] = m_events[i - 3][2] + 10\n",
    "            invis_unmasked += 1\n",
    "\n",
    "print(f'Swapped {vis_masked} masked trials which where rated conscious'\n",
    "      f'\\nSwapped {invis_unmasked} unmasked trials which where rated unconscious')\n",
    "\n",
    "# Save number of swapped triggers\n",
    "swap = [vis_masked,invis_unmasked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34705d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Epochs file for further analysis\n",
    "reject = 180e-6\n",
    "flat = 100e-10\n",
    "\n",
    "epochs = []\n",
    "drops = []\n",
    "for event_key in tqdm(red_event_dict.keys(), desc = 'Epoching data: ', unit = 'Condition'):\n",
    "\n",
    "    event_id = red_event_dict[event_key]\n",
    "    epoch = mne.Epochs(cleaned_raw, m_events, event_id = event_id,\n",
    "                                              tmin = -0.2, tmax = 0.7,\n",
    "                                              baseline = None,\n",
    "                                              reject_tmin = 0.3, flat = {'eeg': flat}, reject = {'eeg': reject},\n",
    "                                              preload = False, verbose = False)\n",
    "\n",
    "    # Get number of Trials before and after dropping\n",
    "    N = len(epoch.events)\n",
    "    epoch.drop_bad(verbose = False)\n",
    "    n = len(epoch.events)\n",
    "\n",
    "    epochs.append(epoch)\n",
    "    drops.append(N - n)\n",
    "\n",
    "    # Save condition epoch to local\n",
    "    os.makedirs(main_path + '/' + subj + '/', exist_ok = True)\n",
    "    epoch.save(main_path + '/' + subj + '/' + subj + event_key + '-epo.fif', overwrite = True, verbose = False)\n",
    "\n",
    "print('Number of dropped trials: ', drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add comment regarding preprocessing results\n",
    "comment = '\\'Strong AC corruption\\''\n",
    "bandpass = [2.0,100.0]\n",
    "notches = [50,100]\n",
    "# Reformat SubID\n",
    "fsubj = f'\\'{subj}\\''\n",
    "\n",
    "filters = {'bandpass': bandpass, 'notches': notches}\n",
    "\n",
    "# Create row to append in dataframe\n",
    "sub_d = [fsubj, filters, bad_ICA, bad_CH, swap, reject, flat, drops, comment]\n",
    "\n",
    "proc_info = pd.read_csv(main_path + '/proc_info.csv', sep = ';')\n",
    "proc_info.loc[-1] = sub_d\n",
    "print(proc_info)\n",
    "\n",
    "proc_info.to_csv(main_path + '/proc_info.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eae010",
   "metadata": {},
   "source": [
    "##  Check Evoked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ff988",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, epoch in enumerate(epochs):\n",
    "\n",
    "    mean = epoch.average()\n",
    "    std = epoch.standard_error()\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize = (20,7), dpi = 300)\n",
    "\n",
    "    mean.plot(axes = axs[0], show = False)\n",
    "    std.plot(axes = axs[1], show = False)\n",
    "\n",
    "    condition = list(red_event_dict.keys())[i]\n",
    "\n",
    "    fig.suptitle('Condition key: ' + condition)\n",
    "\n",
    "    axs[0].set_title('Average over trials')\n",
    "    axs[1].set_title('Standard Error over trials')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get exact timepoints of epochs\n",
    "evoked = epochs.average()\n",
    "\n",
    "ts = evoked.get_data()\n",
    "\n",
    "print(f'Extracted events time series have {ts.shape[1]} time points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b44d0",
   "metadata": {},
   "source": [
    "#   Automated preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne  # For EEG/MEG data processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mne.io import read_raw_edf, read_raw_fif, read_raw_bdf\n",
    "from mne.preprocessing import ICA\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "#Our Very Big Dictionary\n",
    "from init import get_maind\n",
    "\n",
    "maind = get_maind()\n",
    "\n",
    "def ion():\n",
    "    mne.viz.set_browser_backend('qt', verbose = True)\n",
    "    return\n",
    "\n",
    "def ioff():\n",
    "    mne.viz.set_browser_backend('matplotlib', verbose = False)\n",
    "    return\n",
    "\n",
    "ica = ICA(n_components=30, random_state=42, max_iter='auto', verbose = False)\n",
    "\n",
    "# Iterable processing function\n",
    "def it_process(i: int):\n",
    "\n",
    "    # Suppress warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        raw = mne.io.read_raw_brainvision(instructions['file_paths'][i], preload = True, verbose = False)\n",
    "        #post = mne.io.read_raw_brainvision(instructions['file_paths'][i], preload = True, verbose = False)\n",
    "\n",
    "    # Detect breaks for removal\n",
    "    break_annots = mne.preprocessing.annotate_break(\n",
    "    raw = raw,\n",
    "    min_break_duration = 20,  # consider segments of at least 20 s duration\n",
    "    t_start_after_previous = 2,  # start annotation 2 s after end of previous one\n",
    "    t_stop_before_next = 2,  # stop annotation 2 s before beginning of next one\n",
    "    verbose = False)\n",
    "\n",
    "    # Create complementary annotation\n",
    "    n_break_annots_onsets = [break_annots[i]['onset'] + break_annots[i]['duration'] for i in range(0,len(break_annots))]\n",
    "\n",
    "    n_break_annots_durations = [break_annots[i + 1]['onset'] - n_break_annots_onsets[i] for i in range(0,len(break_annots) - 1)] + [0]\n",
    "\n",
    "    # Delete segments too short for the notch filter\n",
    "    for j, dur in enumerate(n_break_annots_durations):\n",
    "\n",
    "        if dur < 6.6:\n",
    "            n_break_annots_onsets.pop(j)\n",
    "            n_break_annots_durations.pop(j)\n",
    "\n",
    "    n_break_annots = mne.Annotations(onset = n_break_annots_onsets,\n",
    "                                     duration = n_break_annots_durations,\n",
    "                                     description = break_annots[0]['description'],\n",
    "                                     orig_time = break_annots[0]['orig_time'],\n",
    "                                     extras = None)\n",
    "\n",
    "    if len(n_break_annots) != 0:\n",
    "\n",
    "        raw.set_annotations(raw.annotations + n_break_annots)\n",
    "\n",
    "        raw_segments = raw.crop_by_annotations(n_break_annots)\n",
    "\n",
    "        raw_cropped = mne.concatenate_raws(raw_segments)\n",
    "\n",
    "        del raw_segments\n",
    "\n",
    "    else:\n",
    "\n",
    "        raw_cropped = raw.copy()\n",
    "\n",
    "    del raw\n",
    "\n",
    "    # Remove inherited annotations (There has to be a better way...)\n",
    "    delete = []\n",
    "    for j in range(0,len(raw_cropped.annotations)):\n",
    "\n",
    "        if raw_cropped.annotations[j]['description'] == 'BAD_break':\n",
    "            delete.append(j)\n",
    "\n",
    "    raw_cropped.annotations.delete(delete)\n",
    "\n",
    "    # Apply filters\n",
    "\n",
    "    post = raw_cropped.copy()\n",
    "\n",
    "    del raw_cropped\n",
    "\n",
    "    # Bandpass\n",
    "    post.filter(l_freq = instructions['filters']['bandpass'][0], h_freq = instructions['filters']['bandpass'][1], verbose = False);\n",
    "\n",
    "    # Remove AC interference\n",
    "    post.notch_filter(freqs = instructions['filters']['notches'], verbose = False)\n",
    "\n",
    "    # Apply ICA if components are specified\n",
    "    if instructions['ICA'][i] != None:\n",
    "\n",
    "        ica.fit(post, verbose = False)\n",
    "\n",
    "        ica.exclude = instructions['ICA'][i]\n",
    "\n",
    "        ica.apply(post, verbose = False)\n",
    "\n",
    "    # Extract event markers\n",
    "    events, event_dict = mne.events_from_annotations(post, verbose = False)\n",
    "\n",
    "    # Create copy of events array\n",
    "    m_events = np.copy(events)\n",
    "\n",
    "    swap = None\n",
    "\n",
    "    if instructions['swap'] == True:\n",
    "\n",
    "        vis_masked = 0\n",
    "        invis_unmasked = 0\n",
    "        for j, event in enumerate(events):\n",
    "\n",
    "            # If visible\n",
    "            if event[2] in [43]:\n",
    "                if m_events[j - 3][2] in [11,12,13,14]:\n",
    "                    m_events[j - 3][2] = m_events[j - 3][2] - 10\n",
    "                    vis_masked += 1\n",
    "\n",
    "            # If not visible\n",
    "            elif event[2] in [40]:\n",
    "                if m_events[j - 3][2] in [1,2,3,4]:\n",
    "                    m_events[j - 3][2] = m_events[j - 3][2] + 10\n",
    "                    invis_unmasked += 1\n",
    "\n",
    "        # Save number of swapped triggers\n",
    "        swap = [vis_masked,invis_unmasked]\n",
    "\n",
    "    # Create new directory\n",
    "    new_path = maind['path'] + f'data/{instructions['newcode']}/'\n",
    "\n",
    "    # Save and drop Trials data as Epochs files\n",
    "    drops = []\n",
    "    for event_key in instructions['events'].keys():\n",
    "\n",
    "        event_id = instructions['events'][event_key]\n",
    "        epoch = mne.Epochs(post, m_events, event_id = event_id,\n",
    "                                                tmin = -0.2, tmax = 0.7,\n",
    "                                                baseline = None,\n",
    "                                                reject_tmin = instructions['rejects_tmins'][i], flat = {'eeg': instructions['flats'][i]}, reject = {'eeg': instructions['rejects'][i]},\n",
    "                                                preload = False, verbose = False)\n",
    "\n",
    "        # Get number of Trials before and after dropping\n",
    "        N = len(epoch.events)\n",
    "        epoch.drop_bad(verbose = False)\n",
    "        n = len(epoch.events)\n",
    "\n",
    "        drops.append(N - n)\n",
    "\n",
    "        # Save condition epoch to local\n",
    "        os.makedirs(new_path + instructions['subIDs'][i] + '/', exist_ok = True)\n",
    "        epoch.save(new_path + instructions['subIDs'][i] + '/' + instructions['subIDs'][i] + event_key + '-epo.fif', verbose = False, overwrite = True)\n",
    "\n",
    "        del epoch\n",
    "    del post\n",
    "\n",
    "    # Add info to Dataframe\n",
    "    comment = '\\'none\\''\n",
    "\n",
    "    # Reformat SubID\n",
    "    fsubj = f'\\'{instructions['subIDs'][i]}\\''\n",
    "\n",
    "    # Create row to append in dataframe\n",
    "    sub_d = [fsubj, instructions['filters'], instructions['ICA'][i], [], swap, instructions['rejects_tmins'][i], instructions['rejects'][i], instructions['flats'][i], drops, comment]\n",
    "\n",
    "    #print(f'Subject {instructions['subIDs'][i]} done!')\n",
    "\n",
    "    return sub_d\n",
    "\n",
    "def auto_process(maind: dict, instructions: dict):\n",
    "\n",
    "    # Unprocessed data path\n",
    "    source_path = maind[instructions['exp_name']]['directories']['rw_data']\n",
    "\n",
    "    # Subject IDs\n",
    "    subIDs = maind[instructions['exp_name']]['subIDs']\n",
    "\n",
    "    # Subject unprocessed raw header location\n",
    "    file_paths = [source_path[:-1] + 'Unprocessed/subj' + subj + '.vhdr' for subj in subIDs]\n",
    "\n",
    "    # Add lists to instructions for iteration\n",
    "    instructions['subIDs'] = subIDs\n",
    "    instructions['file_paths'] = file_paths\n",
    "\n",
    "    iters = [i for i in range(0,len(subIDs))]\n",
    "\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    workers = instructions['MP_workers']\n",
    "    chunksize = 1\n",
    "\n",
    "    with Pool(workers, maxtasksperchild = 1) as p:\n",
    "    \n",
    "        results = list(tqdm(p.imap(it_process, iters, chunksize = chunksize),\n",
    "                            desc = 'Processing subjects',\n",
    "                            unit = 'sub',\n",
    "                            total = len(iters),\n",
    "                            leave = True,\n",
    "                            dynamic_ncols = True)\n",
    "                        )\n",
    "\n",
    "    # Create list with processing information\n",
    "    dataf = []\n",
    "    for r in results:\n",
    "\n",
    "        dataf.append(r)\n",
    "\n",
    "    # Create new directory\n",
    "    new_path = maind['path'] + f'data/{instructions['newcode']}/'\n",
    "\n",
    "    # Save info to dataframe\n",
    "    proc_info = pd.DataFrame(dataf, columns = ['SubID','Filter','bad_ICA','bad_CH','TRIG_swap', 'reject_tmin','reject', 'flat', 'bad_Trials','Comment'])\n",
    "\n",
    "    proc_info.to_csv(new_path + 'proc_info.csv', sep = ';', index = False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddbc8d",
   "metadata": {},
   "source": [
    "##   Process without ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Instructions\n",
    "instructions = {\n",
    "    'MP_workers': 2,\n",
    "    'exp_name': 'bmasking_dense',\n",
    "    'newname': 'bmasking_dense_noICA',\n",
    "    'newcode': 'BMDnoICA',\n",
    "    'ch_drops': ['Iz'],\n",
    "    'filters': {'bandpass': [2.0,100.0], 'notches': [50,100]},\n",
    "    'ICA': [None for i in range(0,36)],\n",
    "    'events': {'S__1': 1,\n",
    "               'S__2': 2,\n",
    "               'S__3': 3,\n",
    "               'S__4': 4,\n",
    "               'S_11': 11,\n",
    "               'S_12': 12,\n",
    "               'S_13': 13,\n",
    "               'S_14': 14},\n",
    "    'tlims': (-0.2,0.7),\n",
    "    'show': False,\n",
    "    'swap': True,\n",
    "    'rejects_tmins': [0.3 for i in range(0,36)],\n",
    "    'flats':[100e-10 for i in range(0,36)],\n",
    "    'rejects':[300e-6 for i in range(0,36)]\n",
    "}\n",
    "\n",
    "auto_process(maind = maind, instructions = instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3befd",
   "metadata": {},
   "source": [
    "##  Process with way too much ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Instructions\n",
    "instructions = {\n",
    "    'MP_workers': 2,\n",
    "    'exp_name': 'bmasking_dense',\n",
    "    'newname': 'bmasking_dense_highICA',\n",
    "    'newcode': 'BMDhighICA',\n",
    "    'ch_drops': ['Iz'],\n",
    "    'filters': {'bandpass': [2.0,100.0], 'notches': [50,100]},\n",
    "    'ICA': [[0,1,2,3,26,27,28,29] for i in range(0,36)],\n",
    "    'events': {'S__1': 1,\n",
    "               'S__2': 2,\n",
    "               'S__3': 3,\n",
    "               'S__4': 4,\n",
    "               'S_11': 11,\n",
    "               'S_12': 12,\n",
    "               'S_13': 13,\n",
    "               'S_14': 14},\n",
    "    'tlims': (-0.2,0.7),\n",
    "    'show': False,\n",
    "    'swap': True,\n",
    "    'rejects_tmins': [0.3 for i in range(0,36)],\n",
    "    'flats':[100e-10 for i in range(0,36)],\n",
    "    'rejects':[200e-6 for i in range(0,36)]\n",
    "}\n",
    "\n",
    "auto_process(maind = maind, instructions = instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61859b15",
   "metadata": {},
   "source": [
    "#   Alessio's Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13912e14",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_with_tolerance(list1, list2, tol):\n",
    "    \"\"\"\n",
    "    For each element in list1, find a unique element in list2 such that\n",
    "    abs(list1[i] - list2[j]) <= tol. If multiple candidates exist,\n",
    "    pick the one with minimal |difference|. Each element in list2 can\n",
    "    be matched at most once.\n",
    "\n",
    "    Returns a list `matches` of length len(list1), where matches[i] is\n",
    "    the index j in list2 matched to list1[i], or None if no match.\n",
    "    \"\"\"\n",
    "    matches = [None] * len(list1)\n",
    "    used    = set()  # keep track of already-matched indices in list2\n",
    "\n",
    "    for i, x in enumerate(list1):\n",
    "        best_j    = None\n",
    "        best_diff = tol + 1e-12\n",
    "\n",
    "        # scan through list2 to find the closest unused candidate\n",
    "        for j, y in enumerate(list2):\n",
    "            if j in used:\n",
    "                continue\n",
    "            diff = abs(x - y)\n",
    "            if diff <= tol and diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_j    = j\n",
    "\n",
    "        if best_j is not None:\n",
    "            used.add(best_j)\n",
    "        matches[i] = best_j\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afde955-f678-458c-ba82-3d2ac3d08946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_e = 1001 # code for the start of a netural trial\n",
    "# start_n = 1002 # code for the start of an emotion trial\n",
    "# resp_e = 1005\n",
    "# resp_n = 1005\n",
    "# end_e = 1007\n",
    "# end_n = 1006\n",
    "# confs = [41,42,43,44,51,52,53,54]\n",
    "# cross_switch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c49010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # neutral trial\n",
    "# # The control time (the time of the switch of the cross) as to be matched with the response time of the face trials\n",
    "# data = cleaned_raw\n",
    "# # data = raw_filtered\n",
    "\n",
    "\n",
    "# cond_codes = [(start_e, resp_e, end_e),(start_n, resp_n, end_n)]\n",
    "\n",
    "# trials_dict = {'trial': [], 'trial_norm': [], 'baseline':[],'baseline_norm':[], 'resp_point': [], 'label': [], 'confidence':[], 'crosstime':[],\n",
    "#                'matching':[]}\n",
    "# for c, cond in enumerate(['real','contr']):# Loop over the two classes\n",
    "#     print(cond)\n",
    "#     start = cond_codes[c][0]\n",
    "#     resp = cond_codes[c][1]\n",
    "#     end = cond_codes[c][2]\n",
    "#     where_start = np.where(events[:,2]==start)\n",
    "#     for w in tqdm(where_start[0]): # Loop over the start of each trial\n",
    "#         start_point = events[w][0]\n",
    "#         i=1\n",
    "#         itsended = False # the current trial has an end\n",
    "#         control=False\n",
    "#         resp_point=0\n",
    "#         conf_val = None\n",
    "#         control_time = None\n",
    "#         while(w+i!=len(events)): # Loop over the successive sample after the start, looking for the end\n",
    "#             curr_samp = events[w+i]\n",
    "#             ##### DECOMMENTARE CON I TRIGGER NORMALI ####\n",
    "#             # if curr_samp[2]==cross_switch and c==1:\n",
    "#             #     # control = True\n",
    "#             #     control_time = curr_samp[0]\n",
    "#             if curr_samp[2]==start_n or curr_samp[2]==start_e: # The end of the trial is missing, discard the trial\n",
    "#                 break\n",
    "#             if curr_samp[2]==resp:\n",
    "#                 if resp_point==0:\n",
    "#                     itsended=True\n",
    "#                     resp_point=curr_samp[0]\n",
    "#                     end_point=curr_samp[0]\n",
    "#             if curr_samp[2]==end:\n",
    "#                 end_point=curr_samp[0]\n",
    "#                 itsended=True\n",
    "#             if curr_samp[2] in confs:\n",
    "#                 conf_val = curr_samp[-1]\n",
    "#                 break\n",
    "#             i+=1\n",
    "\n",
    "#         if itsended:\n",
    "#             resp_point = resp_point-start_point\n",
    "#             if c==0: # Face\n",
    "#                 if resp_point>0:\n",
    "#                     # I take the activity from -150 to -50 as a baseline (no stimulation)\n",
    "#                     extracted_data = data.get_data(start=start_point-100, stop=end_point)\n",
    "                    \n",
    "#                     base = extracted_data[:, :100]\n",
    "#                     # print(base.shape)\n",
    "#                     mean_base = base.mean(axis=1)\n",
    "#                     extracted_data_scale = extracted_data.T-mean_base\n",
    "#                     # print(extracted_data_scale.shape)\n",
    "#                     extracted_data_norm = extracted_data_scale/extracted_data_scale.std() # z-score the trial\n",
    "#                     extracted_data_scale = extracted_data_scale.T\n",
    "#                     extracted_data_norm = extracted_data_norm.T\n",
    "                    \n",
    "#                     trials_dict['trial'].append(extracted_data_scale[:, 100:])\n",
    "#                     trials_dict['trial_norm'].append(extracted_data_norm[:, 100:])\n",
    "#                     trials_dict['baseline'].append(extracted_data_scale[:, :100])\n",
    "#                     trials_dict['baseline_norm'].append(extracted_data_norm[:, :100])\n",
    "#                     trials_dict['resp_point'].append(resp_point)\n",
    "#                     trials_dict['label'].append(cond)\n",
    "#                     trials_dict['confidence'].append(conf_val)\n",
    "#                     trials_dict['crosstime'].append(None)\n",
    "#             elif c==1: # Control\n",
    "#                 if resp_point>0:\n",
    "#                     # I take the activity from -150 to -50 as a baseline (no stimulation)\n",
    "#                     extracted_data = data.get_data(start=start_point-100, stop=end_point)\n",
    "                    \n",
    "#                     base = extracted_data[:, :100]\n",
    "#                     # print(base.shape)\n",
    "#                     mean_base = base.mean(axis=1)\n",
    "#                     extracted_data_scale = extracted_data.T-mean_base\n",
    "#                     # print(extracted_data_scale.shape)\n",
    "#                     extracted_data_norm = extracted_data_scale/extracted_data_scale.std() # z-score the trial\n",
    "#                     extracted_data_scale = extracted_data_scale.T\n",
    "#                     extracted_data_norm = extracted_data_norm.T\n",
    "#                     ##### DECOMMENTARE CON I TRIGGER NORMALI ####\n",
    "#                     # control_time = control_time-start_point +time_adjuster\n",
    "#                     # if control_time[0]>0:\n",
    "#                     ##### DEINDENTARE CON I TRIGGER NORMALI #####\n",
    "#                     trials_dict['trial'].append(extracted_data_scale[:, 100:])\n",
    "#                     trials_dict['trial_norm'].append(extracted_data_norm[:, 100:])\n",
    "#                     trials_dict['baseline'].append(extracted_data_scale[:, :100])\n",
    "#                     trials_dict['baseline_norm'].append(extracted_data_norm[:, :100])\n",
    "#                     trials_dict['resp_point'].append(resp_point)\n",
    "#                     trials_dict['label'].append(cond)\n",
    "#                     trials_dict['confidence'].append(conf_val)\n",
    "#                     # trials_dict['crosstime'].append(control_time[0])\n",
    "#                     trials_dict['crosstime'].append('cane')\n",
    "\n",
    "                        \n",
    "                \n",
    "    \n",
    "    \n",
    "# print(len(trials_dict['trial']),len(trials_dict['trial_norm']),len(trials_dict['baseline']),len(trials_dict['baseline_norm']),\n",
    "#       len(trials_dict['resp_point']), len(trials_dict['label']),len(trials_dict['confidence']),)\n",
    "# print(trials_dict['trial'][0].shape,trials_dict['trial_norm'][0].shape,trials_dict['baseline'][0].shape,trials_dict['baseline_norm'][0].shape)\n",
    "            \n",
    "\n",
    "\n",
    "#------------- QUELLO VERO DA DECOMMENTARE CON I TRIGGER GIUSTI -------------------------------------#\n",
    "\n",
    "# neutral trial\n",
    "# The control time (the time of the switch of the cross) as to be matched with the response time of the face trials\n",
    "data = cleaned_raw\n",
    "# data = raw_filtered\n",
    "\n",
    "\n",
    "cond_codes = [(start_e, resp_e, end_e),(start_n, resp_n, end_n)]\n",
    "\n",
    "trials_dict = {'trial': [], 'trial_norm': [], 'baseline':[],'baseline_norm':[], 'resp_point': [], 'label': [], 'confidence':[], 'crosstime':[],\n",
    "               'matching':[]}\n",
    "for c, cond in enumerate(['real','contr']):# Loop over the two classes\n",
    "    print(cond)\n",
    "    start = cond_codes[c][0]\n",
    "    resp = cond_codes[c][1]\n",
    "    end = cond_codes[c][2]\n",
    "    where_start = np.where(events[:,2]==start)\n",
    "    for w in tqdm(where_start[0]): # Loop over the start of each trial\n",
    "        start_point = events[w][0]\n",
    "        i=1\n",
    "        itsended = False # the current trial has an end\n",
    "        control=False\n",
    "        resp_point=0\n",
    "        while(w+i!=len(events)): # Loop over the successive sample after the start, looking for the end\n",
    "            curr_samp = events[w+i]\n",
    "            if curr_samp[2]==cross_switch and c==1:\n",
    "                # control = True\n",
    "                control_time = curr_samp[0]\n",
    "            if curr_samp[2]==start_n or curr_samp[2]==start_e: # The end of the trial is missing, discard the trial\n",
    "                break\n",
    "            if curr_samp[2]==resp:\n",
    "                if resp_point==0:\n",
    "                    itsended=True\n",
    "                    resp_point=curr_samp[0]\n",
    "                    end_point=curr_samp[0]\n",
    "            if curr_samp[2]==end:\n",
    "                end_point=curr_samp[0]\n",
    "                itsended=True\n",
    "            if curr_samp[2] in confs:\n",
    "                conf_val = curr_samp[-1]\n",
    "                break\n",
    "            i+=1\n",
    "\n",
    "        if itsended:\n",
    "            resp_point = resp_point-start_point\n",
    "            if c==0: # Face\n",
    "                if resp_point>0:\n",
    "                    # I take the activity from -150 to -50 as a baseline (no stimulation)\n",
    "                    extracted_data = data.get_data(start=start_point-100, stop=end_point)\n",
    "                    \n",
    "                    base = extracted_data[:, :100]\n",
    "                    # print(base.shape)\n",
    "                    mean_base = base.mean(axis=1)\n",
    "                    extracted_data_scale = extracted_data.T-mean_base\n",
    "                    # print(extracted_data_scale.shape)\n",
    "                    extracted_data_norm = extracted_data_scale/extracted_data_scale.std() # z-score the trial\n",
    "                    extracted_data_scale = extracted_data_scale.T\n",
    "                    extracted_data_norm = extracted_data_norm.T\n",
    "                    \n",
    "                    trials_dict['trial'].append(extracted_data_scale[:, 100:])\n",
    "                    trials_dict['trial_norm'].append(extracted_data_norm[:, 100:])\n",
    "                    trials_dict['baseline'].append(extracted_data_scale[:, :100])\n",
    "                    trials_dict['baseline_norm'].append(extracted_data_norm[:, :100])\n",
    "                    trials_dict['resp_point'].append(resp_point)\n",
    "                    trials_dict['label'].append(cond)\n",
    "                    trials_dict['confidence'].append(conf_val)\n",
    "                    trials_dict['crosstime'].append(None)\n",
    "            elif c==1: # Control\n",
    "                if resp_point>0:\n",
    "                    # I take the activity from -150 to -50 as a baseline (no stimulation)\n",
    "                    extracted_data = data.get_data(start=start_point-100, stop=end_point)\n",
    "                    \n",
    "                    base = extracted_data[:, :100]\n",
    "                    # print(base.shape)\n",
    "                    mean_base = base.mean(axis=1)\n",
    "                    extracted_data_scale = extracted_data.T-mean_base\n",
    "                    # print(extracted_data_scale.shape)\n",
    "                    extracted_data_norm = extracted_data_scale/extracted_data_scale.std() # z-score the trial\n",
    "                    extracted_data_scale = extracted_data_scale.T\n",
    "                    extracted_data_norm = extracted_data_norm.T\n",
    "                    control_time = control_time-start_point +time_adjuster\n",
    "                    # if control_time[0]>0:\n",
    "                    if control_time>0:\n",
    "\n",
    "                    \n",
    "                        trials_dict['trial'].append(extracted_data_scale[:, 100:])\n",
    "                        trials_dict['trial_norm'].append(extracted_data_norm[:, 100:])\n",
    "                        trials_dict['baseline'].append(extracted_data_scale[:, :100])\n",
    "                        trials_dict['baseline_norm'].append(extracted_data_norm[:, :100])\n",
    "                        trials_dict['resp_point'].append(resp_point)\n",
    "                        trials_dict['label'].append(cond)\n",
    "                        trials_dict['confidence'].append(conf_val)\n",
    "                        trials_dict['crosstime'].append(control_time[0])\n",
    "                        # trials_dict['crosstime'].append(control_time)\n",
    "\n",
    "                        \n",
    "                \n",
    "    \n",
    "    \n",
    "print(len(trials_dict['trial']),len(trials_dict['trial_norm']),len(trials_dict['baseline']),len(trials_dict['baseline_norm']),\n",
    "      len(trials_dict['resp_point']), len(trials_dict['label']),len(trials_dict['confidence']),)\n",
    "print(trials_dict['trial'][0].shape,trials_dict['trial_norm'][0].shape,trials_dict['baseline'][0].shape,trials_dict['baseline_norm'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4e7f6-7081-4498-b354-80b63e0a62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# N_tr = np.sum(np.asarray(trials_dict['crosstime'])==None)\n",
    "\n",
    "# list2 = np.array(np.asarray(trials_dict['resp_point'])[np.asarray(trials_dict['crosstime'])!=None]) # control\n",
    "# list1 = np.asarray(trials_dict['resp_point'])[np.asarray(trials_dict['crosstime'])==None] # Face\n",
    "# trials_dict['matching'] = np.full((len(list1)), -999)\n",
    "# tol   = 600\n",
    "\n",
    "# matches = match_with_tolerance(list1, list2, tol)\n",
    "# for i, j in enumerate(matches):\n",
    "#     if j is None:\n",
    "#         print(f\"list1[{i}] = {list1[i]:.2f} → no match\")\n",
    "#     else:\n",
    "#         print(f\"list1[{i}] = {list1[i]:.2f} ↔ list2[{j}] = {list2[j]:.2f}\")\n",
    "\n",
    "# for l1,l2 in enumerate(matches):\n",
    "#     # print(l1,l2)\n",
    "#     if l1!=None and l2!=None:\n",
    "#         trials_dict['matching'][l1] = l2+N_tr\n",
    "#         # trials_dict['matching'][l2+N_tr] = l1\n",
    "    \n",
    "\n",
    "# np.sum(trials_dict['matching']>=0)\n",
    "\n",
    "#------------- QUELLO VERO DA DECOMMENTARE CON I TRIGGER GIUSTI -------------------------------------#\n",
    "\n",
    "\n",
    "# Example usage\n",
    "N_tr = np.sum(np.asarray(trials_dict['crosstime'])==None) # Number of face trials\n",
    "\n",
    "list2 = np.array(np.asarray(trials_dict['resp_point'])[np.asarray(trials_dict['crosstime'])!=None]) # control\n",
    "list2_rt = np.array(np.asarray(trials_dict['resp_point'])[np.asarray(trials_dict['crosstime'])!=None]) # control\n",
    "list1 = np.asarray(trials_dict['resp_point'])[np.asarray(trials_dict['crosstime'])==None] # Face\n",
    "trials_dict['matching'] = np.full((len(list1)), -999)\n",
    "tol   = 600\n",
    "\n",
    "matches = match_with_tolerance(list1, list2, tol)\n",
    "for i, j in enumerate(matches):\n",
    "    if j is None:\n",
    "        print(f\"list1[{i}] = {list1[i]:.2f} → no match\")\n",
    "    else:\n",
    "        print(f\"list1[{i}] = {list1[i]:.2f} ↔ list2[{j}] = {list2_rt[j]:.2f}\")\n",
    "\n",
    "for l1,l2 in enumerate(matches):\n",
    "    # print(l1,l2)\n",
    "    if l1!=None and l2!=None:\n",
    "        trials_dict['matching'][l1] = l2+N_tr\n",
    "        # trials_dict['matching'][l2+N_tr] = l1\n",
    "    \n",
    "\n",
    "np.sum(trials_dict['matching']>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06241ed-85b4-43f5-b0ea-909f9e5354a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trials_dict['matching'], len(trials_dict['matching']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6578b-d1b0-4e00-b591-a2a1463644d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the non-matched trials and controls\n",
    "print('Controls')\n",
    "for i in range(N_tr):\n",
    "    if i not in matches:\n",
    "        print(trials_dict['crosstime'][N_tr:][i])\n",
    "print('Face')\n",
    "for i,m in enumerate(matches):\n",
    "    if m==None:\n",
    "        print(trials_dict['resp_point'][:N_tr][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b8e67-f20a-414c-bfe4-b3f3324e77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Breaking ###\n",
    "if breaking:\n",
    "    with open('D:/PhD/CFS_eeg/data/new_exp/prep_task/breaking/subj_'+subj+'_trials_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(trials_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "### Reverse ###\n",
    "elif reverse:\n",
    "    with open('D:/PhD/CFS_eeg/data/new_exp/prep_task/reverse/subj_'+subj+'_rev_trials_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(trials_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a93c4-e901-4406-b481-0aa5a6683637",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =plt.figure()\n",
    "plt.hist(trials_dict['resp_point'][:142], alpha=0.4, bins=20);\n",
    "plt.hist(np.array(trials_dict['crosstime'])[np.array(trials_dict['crosstime'])!=None], alpha=0.5, bins=20);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0e735-7eff-435e-8d63-93799d96570c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99714e5f-b6a8-4976-8c54-be4167b589c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
